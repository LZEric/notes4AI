满血版，集团使用，200～300w
14B，部门使用，10w左右
7B，个人使用，不属于MOE架构，为Dense架构
Deepseek-V3 文科生，代码这个就够了
Deepseek-R1 理科生，处理较为复杂的数学、物理等问题
Deepseek-R1-Zero 纯强化学习 

*蒸馏*：让大模型输出结果，拿这个结果给小模型训练
*低秩*：快速矩阵计算的常用方法，对KV进行压缩（之前很少用）
*监督学习*：问题+答案去学习，人类标注
*强化学习*：问题+推理答案，机器自学习

## Deepseek技术上的创新
#### MLA
低秩键值联合压缩的注意力机制，显著减小KV缓存的同时提高计算效率
#### MoE
混合专家，对处理的任务进行分流，使得每次训练或者推理只激活了很少的链路，降低成本提高效率。每个专家都是一个LLMmini
#### 混合精度框架
在不同的区块使用不同的精度来存储数据，低的用FP8（精度越高占用内存越大）
#### 强化学习+长联路推理
思维链长度可以上万字

## 本地部署
### vllm（高性能部署框架，可以提升每秒token数）
1. 在modelscope上，选择合适的大模型
2. 下载，代码如下
```
from modelscope import snapshot_download
snapshot_download('大模型标识', cache_dir='/root/autodl-tmp/models')
```
建议下载到/root/autodl-tmp
### Ollama
1. 安装Ollama
2. 执行命令
```
ollama pull 模型id
ollama run 模型id
```

## 提示词
#### 通用模型
1. 需要逐步指导
2. 给明确的指导
#### 推理模型
1. 提示语更简洁，无需逐步指导，如果强行指定返回会降低推理能力

#### 提示次重要性
任务〉上下文〉示例〉角色〉格式〉语气
重要的可以放开头或者结尾
使用大模型对提示词进行优化
结构化提示词
```
1. instruction 指令
2. CoT 步骤
3. reasoning summary
4. final answer
```
添加下面的分隔符，可以告知大模型示例
```
"""
"""
```

作业
1. ollama部署
2. prompt.py跑通
